import requests
import json
import sys
import time


apikey = "4d8186d5f33938ebf6f639a630d56da64ac57c93c6c2d6c6f57d870dfdd5ccae"
base_url = "https://www.virustotal.com/api/v3/urls"
analysis_url = "https://www.virustotal.com/api/v3/analyses/"
results_list = []


def scan_url (api_key, urls_to_scan, base_api_url, analysis_api_url):

    num_potentially_malicious = 0
    num_safe = 0
    for url in urls_to_scan:
        
        #Scan for the url id
        id_response = scan_api(api_key, url, base_api_url)
        scan_json = json.loads(id_response.text)
        url_id = scan_json['data']['id']


        #Queries the url analysis
        analysis_api(api_key, url_id, analysis_api_url, True)
        time.sleep(10)
        #Gets the url analysis object back
        #This is the nature of the api, i'm sure there's a way around this, but this is how i'm doing that
        analysis_response = analysis_api(api_key, url_id, analysis_api_url)
        scan_response_json = json.loads(analysis_response.text)

        #Checks if any of the virus scanner picked up any malicious or suspicious activity on the url
        mal_list = []
        sus_list = []
        if scan_response_json['data']['attributes']['stats']['malicious'] > 0 or scan_response_json['data']['attributes']['stats']['suspicious'] > 0:
            num_potentially_malicious += 1
            mal_list, sus_list = get_scan_methods(scan_response_json)
            print(f"Scans found {url} to be bad according to mal_list:{mal_list} and sus_list{sus_list}")
        else:
            num_safe += 1
            print(f"Scans found no malicious or suspicious flags for {url}")

        #time.sleep(25)
        results_dict = {"scanned_url" : scan_response_json['meta']['url_info']['url'], 
                        "harmless" : scan_response_json['data']['attributes']['stats']['harmless'],
                        "malicious" : scan_response_json['data']['attributes']['stats']['malicious'],
                        "enginesFoundMalicious" : mal_list,
                        "suspicious" : scan_response_json['data']['attributes']['stats']['suspicious'],
                        "enginesFoundSuspicious" : sus_list,
                        "undetected" : scan_response_json['data']['attributes']['stats']['undetected'],
                        "timeout" : scan_response_json['data']['attributes']['stats']['timeout']
                        }
        results_list.append(results_dict)
        print(results_list)
        time.sleep(60)

    return num_potentially_malicious, num_safe


def get_scan_methods(json_object):
    malicious_list = []
    suspicious_list = []
    for result in json_object['data']['attributes']['results']:
        if json_object['data']['attributes']['results'][f'{result}']['category'] == "malicious":
            malicious_list.append(json_object['data']['attributes']['results'][result]['engine_name'])        

        if json_object['data']['attributes']['results'][f'{result}']['category'] == "suspicious":
            suspicious_list.append(json_object['data']['attributes']['results'][result]['engine_name'])
     
    return malicious_list, suspicious_list

#Gets the url_id to be passed into the analysis endpoint
def scan_api(api_key, url, base_api_url):
    payload = { "url": url }
    headers = {
        "accept": "application/json",
        "x-apikey": apikey,
        "content-type": "application/x-www-form-urlencoded"
    }
    response = requests.post(base_api_url, data=payload, headers=headers)
    return response

#Gets the analysis for the url
def analysis_api(api_key, url_id, analysis_api_url, query_flag=False):
    if query_flag == True:
        print("Querying url...")
        return
    else:
        analysis_headers = {
                "accept": "application/json",
                "x-apikey": api_key
            }
        analysis_url_and_id = analysis_api_url + url_id
        analysis_response = requests.get(analysis_url_and_id, headers=analysis_headers)
        return analysis_response

def read_search_results(text_file):
    url_list = []

    with open(text_file, 'r') as file:
        file_content = file.readlines()

    for url in file_content:
        url_list.append(url.strip())

    return url_list



#====================================================#
if __name__ == '__main__':
    urls_to_scan = read_search_results('google_search_results_2023_11_13.txt')
    number_of_urls = len(urls_to_scan)

    num_mal, num_safe = scan_url(apikey, urls_to_scan, base_url, analysis_url)

    file = open('virustotal_results.txt','a')
    file.write(f"Out of {number_of_urls} urls scanned: {num_mal} were found to be malicious, {num_safe} were found to be safe")
    json.dump(results_list, file, indent=2)
    file.close()